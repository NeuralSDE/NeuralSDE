# Experiment name
name: "kth"

# Dataset parameters
data:
  data_root:  ./data/kth/hd5 # path to the dataset folder
  input_size: 64
  crop_size: 64
  frames_per_sample: 40
  skip_frames: 0
  random_horizontal_flip: False
  aug: False
  albumentations: True

# Parameters of the model
model:
  # Defines the sigma min
  sigma: 0.0000001
  func: "d"
  noise_std: 0.1
  num_input_frames: 4
  # Parameters for vector field regressor
  vector_field_regressor:
    state_size: 4
    state_res: [8, 8]
    inner_dim: 768
    depth: 4
    mid_depth: 5
    out_norm: "ln"
    # n_feat: 696
    type: "vit"
  # Parameters for the autoencoder
  autoencoder:
    # The architecture of the autoencoder [ours, ldm-vq, ldm-kl]
    type: "ldm-vq"
    config: "f8_small"
    ckpt_path:  "./model/ae/vqvae_kth.ckpt"

# Parameters for the training
training:
  # Total number of epochs to train the model for
  num_epochs: 100000
  # Parameters for batch building
  batching:
    batch_size: 16
    num_workers: 7

  # Parameters for the optimizer
  optimizer:
    learning_rate: 0.0001
    weight_decay: 0.000005

    num_warmup_steps: 5000
    num_training_steps: 300000

  # Number of observations in the sequence
  num_observations: 40
  # Number of frames to use as initial conditioning
  condition_frames: 4
  # Nuber of frames to generate
  frames_to_generate: 36

  # Parameters for loss weighting
  loss_weights:
    flow_matching_loss: 1.0

# Parameters for the evaluation
evaluation:
  # Parameters for batch building
  batching:
    batch_size: 4
    num_workers: 7

  # Number of observations in the sequence
  num_observations: 40
  # Number of frames to use as initial conditioning
  condition_frames: 4
  # Nuber of frames to generate
  frames_to_generate: 36
